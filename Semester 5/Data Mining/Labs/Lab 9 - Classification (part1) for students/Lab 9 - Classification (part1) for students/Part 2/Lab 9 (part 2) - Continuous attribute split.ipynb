{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d5186-b3f2-4a00-a4a1-67fc4f3570ee",
   "metadata": {},
   "source": [
    "# Lab 9: Continuous attribute spliting (part 2)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Lab 8 (part2) in our Data Mining course! In this lab, we'll be diving into a crucial aspect of data analysis and preprocessing - finding the best continuous split in a dataset. This process is fundamental in various data mining tasks such as decision tree construction, feature selection, and improving the accuracy of our models.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Our primary objective in this lab is to learn how to determine the best split point for a continuous variable in a dataset. We'll be focusing on the concept of **Information Gain**, a measure used in decision tree algorithms to split a node. Information gain helps us understand which split will most effectively help us classify the data.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We'll be working with the 'diabetes.csv' dataset, a widely used dataset in machine learning. This dataset presents a set of variables measured in individuals, some of whom have diabetes.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the concept of information gain and its importance in data mining.\n",
    "- Implement a method to calculate the best split for continuous variables based on information gain.\n",
    "- Apply these concepts to the 'diabetes.csv' dataset to gain practical experience.\n",
    "\n",
    "Let's get started and explore how to make informed decisions using data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e2a0b1-e79c-47f7-8de4-435fc2fdf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf0590-a8e1-4a45-a61d-ea9141b01e0f",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "In this lab, we will be working with the `diabetes.csv` dataset. This dataset is commonly used in the field of data mining and machine learning to illustrate various techniques and algorithms.\n",
    "\n",
    "### Overview of the Dataset\n",
    "\n",
    "The `diabetes.csv` dataset comprises several medical predictor variables and one target variable, `Outcome`. The predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "### Features of the Dataset\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- `Pregnancies`: Number of times pregnant\n",
    "- `Glucose`: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- `BloodPressure`: Diastolic blood pressure (mm Hg)\n",
    "- `SkinThickness`: Triceps skin fold thickness (mm)\n",
    "- `Insulin`: 2-Hour serum insulin (mu U/ml)\n",
    "- `BMI`: Body mass index (weight in kg/(height in m)^2)\n",
    "- `DiabetesPedigreeFunction`: Diabetes pedigree function\n",
    "- `Age`: Age (years)\n",
    "- `Outcome`: Class variable (0 or 1) where 1 denotes the iill be crucial in our analysis.\n",
    "\n",
    "_Note: Ensure that you have the `diabetes.csv` dataset available in your working environment before starting the lab exercises._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a52cae0-3820-4bc5-bf4d-99bf993deab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0              6      148             72             35        0  33.6   \n",
      "1              1       85             66             29        0  26.6   \n",
      "2              8      183             64              0        0  23.3   \n",
      "3              1       89             66             23       94  28.1   \n",
      "4              0      137             40             35      168  43.1   \n",
      "..           ...      ...            ...            ...      ...   ...   \n",
      "763           10      101             76             48      180  32.9   \n",
      "764            2      122             70             27        0  36.8   \n",
      "765            5      121             72             23      112  26.2   \n",
      "766            1      126             60              0        0  30.1   \n",
      "767            1       93             70             31        0  30.4   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                       0.627   50        1  \n",
      "1                       0.351   31        0  \n",
      "2                       0.672   32        1  \n",
      "3                       0.167   21        0  \n",
      "4                       2.288   33        1  \n",
      "..                        ...  ...      ...  \n",
      "763                     0.171   63        0  \n",
      "764                     0.340   27        0  \n",
      "765                     0.245   30        0  \n",
      "766                     0.349   47        1  \n",
      "767                     0.315   23        0  \n",
      "\n",
      "[768 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bea36-d002-4830-9585-b386d441af47",
   "metadata": {},
   "source": [
    "## Entropy Function Explanation\n",
    "\n",
    "The `entropy` function is a crucial part of our information gain calculation. It measures the impurity or randomness in the data, which helps in deciding how a dataset should be split. The entropy of a dataset is calculated using the formula:\n",
    "\n",
    "$$ \\text{Entropy} = -\\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e74852ee-6443-41a1-b172-71c65aed0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = # Fix Me #\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbccf58-d6c6-4319-b59b-a92d951f46ce",
   "metadata": {},
   "source": [
    "## Understanding Information Gain\n",
    "\n",
    "### What is Information Gain?\n",
    "\n",
    "Information Gain is a measure used in decision trees and other machine learning algorithms to determine how well a feature splits a set of data. It's a key concept in algorithms like ID3, C4.5, and CART. Information Gain helps in selecting the feature that partitions the data most effectively, essentially dividing the dataset into groups that are as pure as possin Gain\n",
    "\n",
    "The formula for Information Gain is:\n",
    "\n",
    "$$ \\text{Information Gain} = \\text{Entropy (Parent)} - [\\text{Weighted Average} \\times \\text{Entropy (Children)}] $$\n",
    "\n",
    "- **Entropy (Parent)**: This is the measure of uncertainty or disorder before the split. It quantifies how much variation there is in the class label.\n",
    "\n",
    "- **Weighted Average Ã— Entropy (Children)**: This term represents the amount of disorder or uncertainty after the split. The entropy of each subset is calculated and then weighted by the proportion of the total dataset that each subset represents.\n",
    "\n",
    "The goal is to maximize the information gain, which implies a greater reduction in uncertainty after the split.\n",
    "\n",
    "### Parameters of the InfoGain Function\n",
    "\n",
    "- `data`: The dataset on which the information gain calculation is to be performed. It should include both the feature columns and the target column.\n",
    "\n",
    "- `split_column`: The name of the column in the dataset based on which the split is to be calculated. This column is the feature on which the decision to split will be based.\n",
    "\n",
    "- `split_value`: The value in the `split_column` that is used to divide the data into two groups. Data where the value in `split_column` is less than or equal to `split_value` forms one group, while the rest forms another.\n",
    "\n",
    "- `target_name`: The name of the target variable in the dataset. This is the variable for which the purity increase (or entropy dec of computational resources.\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9105109-ccc6-4ab8-982a-7496e5e4b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(data,split_column, split_value, target_name=\"Outcome\"):\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    lower_group = data.where(data[split_column] <= split_value).dropna()\n",
    "    upper_group = # Fix Me #\n",
    "    lower_entropy = # Fix Me #\n",
    "    upper_entropy = # Fix Me #\n",
    "    weighted_entropy = lower_entropy + upper_entropy\n",
    "    information_gain = # Fix Me #\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d5af7-81f5-45c5-9e82-451d5a6c3f2d",
   "metadata": {},
   "source": [
    "## Midpoint Splitting for Any Continuous Variablew\n",
    "\n",
    "Midpoint splitting is a technique used in data mining to determine optimal split points for continuous variabrocess\n",
    "\n",
    "- Define `split_column` as the continuous variable of interest.\n",
    "- Sort its unique values and calculate midpoints between consecutive pairs.\n",
    "- These midpoints serve as potential sple's distribution.\n",
    "'s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "377f4b60-87c8-4650-86ae-e79084f8dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'outcome' is your class variable\n",
    "# Replace 'BMI' with the column name you want to evaluate\n",
    "split_column = 'BMI'\n",
    "unique_values = sorted(df[split_column].unique())\n",
    "midpoints = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411976a-7ffe-48c6-9b53-b84aa79e85cf",
   "metadata": {},
   "source": [
    "## Finding the Optimal Split Pointw\n",
    "\n",
    "The code aims to find the best split point in a dataset for a specified continuous variable, using information gain as the metric.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Initialize `best_split` and `max_info_gain`.\n",
    "2. Iterate over each midpoint calculated from the continuous variable.\n",
    "3. Calculate the information gain for each midpoint using the `InfoGain` function.\n",
    "4. Compare each information gain with the current maximum. If it's higher, update `max_info_gain` and `best_split`.\n",
    "5. Finally, output the best split point and its corresponding inforin the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a830b64c-950c-4d69-81cc-cf834b979960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best split for BMI is at 27.85 with an information gain of 0.07489856064647493\n"
     ]
    }
   ],
   "source": [
    "best_split = None\n",
    "max_info_gain = -1\n",
    "\n",
    "for midpoint in midpoints:\n",
    "    current_info_gain = # Fix Me #\n",
    "    #print(\"midpoint = \",midpoint, \"information gain = \",current_info_gain)\n",
    "    if current_info_gain > max_info_gain:\n",
    "        max_info_gain = current_info_gain\n",
    "        best_split = midpoint\n",
    "\n",
    "print(f\"Best split for BMI is at {best_split} with an information gain of {max_info_gain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f0b3d-1312-4d79-a9c0-547b58f73926",
   "metadata": {},
   "source": [
    "## Exercise: Implementing Gini Index, Misclassification Error Rate, and Gain Calculation\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to implement the Gini index, Misclassification Error Rate, and Gain calculations at home. This exercise will enhance your understanding of different metrics used to evaluate the effectiveness of dataset splits in decision tree algorithms.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Implement the Gini Index Function**:\n",
    "   - Write a function to calculate the Gini index for a given dataset split.\n",
    "   - The function should accept groups of data and the list of classes and return the Gini index.\n",
    "\n",
    "2. **Implement the Misclassification Error Rate Function**:\n",
    "   - Similarly, develop a function to calculate the Misclassification Error Rate for dataset splits.\n",
    "   - This function should also take groups of data and the list of classes as inputs.\n",
    "\n",
    "3. **Implement the Gain Calculation**:\n",
    "   - Create a function to calculate the Gain, using either the Gini index or the Misclassification Error Rate.\n",
    "   - This function should help compare the impurity of the dataset before and after the split.\n",
    "\n",
    "4. **Test Your Implementations**:\n",
    "   - Use a simple dataset to test your functions for both Gini index and Misclassification Error Rate.\n",
    "   - Ensure the correctness of your implementations by cross-checking with theoretical values or using established libraries.\n",
    "\n",
    "This exercise will deepen your practical understanding of key metrics in decision tree algorithms and their impact on model performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
