{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d5186-b3f2-4a00-a4a1-67fc4f3570ee",
   "metadata": {},
   "source": [
    "# Lab 9: Continuous attribute spliting (part 2)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Lab 8 (part2) in our Data Mining course! In this lab, we'll be diving into a crucial aspect of data analysis and preprocessing - finding the best continuous split in a dataset. This process is fundamental in various data mining tasks such as decision tree construction, feature selection, and improving the accuracy of our models.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Our primary objective in this lab is to learn how to determine the best split point for a continuous variable in a dataset. We'll be focusing on the concept of **Information Gain**, a measure used in decision tree algorithms to split a node. Information gain helps us understand which split will most effectively help us classify the data.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We'll be working with the 'diabetes.csv' dataset, a widely used dataset in machine learning. This dataset presents a set of variables measured in individuals, some of whom have diabetes.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the concept of information gain and its importance in data mining.\n",
    "- Implement a method to calculate the best split for continuous variables based on information gain.\n",
    "- Apply these concepts to the 'diabetes.csv' dataset to gain practical experience.\n",
    "\n",
    "Let's get started and explore how to make informed decisions using data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e2a0b1-e79c-47f7-8de4-435fc2fdf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf0590-a8e1-4a45-a61d-ea9141b01e0f",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "In this lab, we will be working with the `diabetes.csv` dataset. This dataset is commonly used in the field of data mining and machine learning to illustrate various techniques and algorithms.\n",
    "\n",
    "### Overview of the Dataset\n",
    "\n",
    "The `diabetes.csv` dataset comprises several medical predictor variables and one target variable, `Outcome`. The predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "### Features of the Dataset\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- `Pregnancies`: Number of times pregnant\n",
    "- `Glucose`: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- `BloodPressure`: Diastolic blood pressure (mm Hg)\n",
    "- `SkinThickness`: Triceps skin fold thickness (mm)\n",
    "- `Insulin`: 2-Hour serum insulin (mu U/ml)\n",
    "- `BMI`: Body mass index (weight in kg/(height in m)^2)\n",
    "- `DiabetesPedigreeFunction`: Diabetes pedigree function\n",
    "- `Age`: Age (years)\n",
    "- `Outcome`: Class variable (0 or 1) where 1 denotes the iill be crucial in our analysis.\n",
    "\n",
    "_Note: Ensure that you have the `diabetes.csv` dataset available in your working environment before starting the lab exercises._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a52cae0-3820-4bc5-bf4d-99bf993deab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'diabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiabetes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead)\n",
      "File \u001b[1;32mc:\\Users\\AMIRA\\.conda\\envs\\DM_ENV\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\AMIRA\\.conda\\envs\\DM_ENV\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\AMIRA\\.conda\\envs\\DM_ENV\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\AMIRA\\.conda\\envs\\DM_ENV\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\AMIRA\\.conda\\envs\\DM_ENV\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bea36-d002-4830-9585-b386d441af47",
   "metadata": {},
   "source": [
    "## Entropy Function Explanation\n",
    "\n",
    "The `entropy` function is a crucial part of our information gain calculation. It measures the impurity or randomness in the data, which helps in deciding how a dataset should be split. The entropy of a dataset is calculated using the formula:\n",
    "\n",
    "$$ \\text{Entropy} = -\\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e74852ee-6443-41a1-b172-71c65aed0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts)) * np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbccf58-d6c6-4319-b59b-a92d951f46ce",
   "metadata": {},
   "source": [
    "## Understanding Information Gain\n",
    "\n",
    "### What is Information Gain?\n",
    "\n",
    "Information Gain is a measure used in decision trees and other machine learning algorithms to determine how well a feature splits a set of data. It's a key concept in algorithms like ID3, C4.5, and CART. Information Gain helps in selecting the feature that partitions the data most effectively, essentially dividing the dataset into groups that are as pure as possin Gain\n",
    "\n",
    "The formula for Information Gain is:\n",
    "\n",
    "$$ \\text{Information Gain} = \\text{Entropy (Parent)} - [\\text{Weighted Average} \\times \\text{Entropy (Children)}] $$\n",
    "\n",
    "- **Entropy (Parent)**: This is the measure of uncertainty or disorder before the split. It quantifies how much variation there is in the class label.\n",
    "\n",
    "- **Weighted Average × Entropy (Children)**: This term represents the amount of disorder or uncertainty after the split. The entropy of each subset is calculated and then weighted by the proportion of the total dataset that each subset represents.\n",
    "\n",
    "The goal is to maximize the information gain, which implies a greater reduction in uncertainty after the split.\n",
    "\n",
    "### Parameters of the InfoGain Function\n",
    "\n",
    "- `data`: The dataset on which the information gain calculation is to be performed. It should include both the feature columns and the target column.\n",
    "\n",
    "- `split_column`: The name of the column in the dataset based on which the split is to be calculated. This column is the feature on which the decision to split will be based.\n",
    "\n",
    "- `split_value`: The value in the `split_column` that is used to divide the data into two groups. Data where the value in `split_column` is less than or equal to `split_value` forms one group, while the rest forms another.\n",
    "\n",
    "- `target_name`: The name of the target variable in the dataset. This is the variable for which the purity increase (or entropy dec of computational resources.\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9105109-ccc6-4ab8-982a-7496e5e4b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(data,split_column, split_value, target_name=\"Outcome\"):\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    lower_group = data.where(data[split_column] <= split_value).dropna()\n",
    "    upper_group = data.where(data[split_column] > split_value).dropna()\n",
    "    lower_entropy = entropy(lower_group[target_name]) * (len(lower_group) / len(data))\n",
    "    upper_entropy = entropy(upper_group[target_name]) * (len(upper_group) / len(data))\n",
    "    weighted_entropy = lower_entropy + upper_entropy\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d5af7-81f5-45c5-9e82-451d5a6c3f2d",
   "metadata": {},
   "source": [
    "## Midpoint Splitting for Any Continuous Variablew\n",
    "\n",
    "Midpoint splitting is a technique used in data mining to determine optimal split points for continuous variabrocess\n",
    "\n",
    "- Define `split_column` as the continuous variable of interest.\n",
    "- Sort its unique values and calculate midpoints between consecutive pairs.\n",
    "- These midpoints serve as potential sple's distribution.\n",
    "'s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "377f4b60-87c8-4650-86ae-e79084f8dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'outcome' is your class variable\n",
    "# Replace 'BMI' with the column name you want to evaluate\n",
    "split_column = 'BMI'\n",
    "unique_values = sorted(df[split_column].unique())\n",
    "midpoints = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411976a-7ffe-48c6-9b53-b84aa79e85cf",
   "metadata": {},
   "source": [
    "## Finding the Optimal Split Pointw\n",
    "\n",
    "The code aims to find the best split point in a dataset for a specified continuous variable, using information gain as the metric.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Initialize `best_split` and `max_info_gain`.\n",
    "2. Iterate over each midpoint calculated from the continuous variable.\n",
    "3. Calculate the information gain for each midpoint using the `InfoGain` function.\n",
    "4. Compare each information gain with the current maximum. If it's higher, update `max_info_gain` and `best_split`.\n",
    "5. Finally, output the best split point and its corresponding inforin the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a830b64c-950c-4d69-81cc-cf834b979960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best split for BMI is at 27.85 with an information gain of 0.07489856064647493\n"
     ]
    }
   ],
   "source": [
    "best_split = None\n",
    "max_info_gain = -1\n",
    "\n",
    "for midpoint in midpoints:\n",
    "    current_info_gain = InfoGain(df,split_column,midpoint)\n",
    "    #print(\"midpoint = \",midpoint, \"information gain = \",current_info_gain)\n",
    "    if current_info_gain > max_info_gain:\n",
    "        max_info_gain = current_info_gain\n",
    "        best_split = midpoint\n",
    "\n",
    "print(f\"Best split for BMI is at {best_split} with an information gain of {max_info_gain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f0b3d-1312-4d79-a9c0-547b58f73926",
   "metadata": {},
   "source": [
    "## Exercise: Implementing Gini Index, Misclassification Error Rate, and Gain Calculation\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to implement the Gini index, Misclassification Error Rate, and Gain calculations at home. This exercise will enhance your understanding of different metrics used to evaluate the effectiveness of dataset splits in decision tree algorithms.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Implement the Gini Index Function**:\n",
    "   - Write a function to calculate the Gini index for a given dataset split.\n",
    "   - The function should accept groups of data and the list of classes and return the Gini index.\n",
    "\n",
    "2. **Implement the Misclassification Error Rate Function**:\n",
    "   - Similarly, develop a function to calculate the Misclassification Error Rate for dataset splits.\n",
    "   - This function should also take groups of data and the list of classes as inputs.\n",
    "\n",
    "3. **Implement the Gain Calculation**:\n",
    "   - Create a function to calculate the Gain, using either the Gini index or the Misclassification Error Rate.\n",
    "   - This function should help compare the impurity of the dataset before and after the split.\n",
    "\n",
    "4. **Test Your Implementations**:\n",
    "   - Use a simple dataset to test your functions for both Gini index and Misclassification Error Rate.\n",
    "   - Ensure the correctness of your implementations by cross-checking with theoretical values or using established libraries.\n",
    "\n",
    "This exercise will deepen your practical understanding of key metrics in decision tree algorithms and their impact on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f88520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Gain: 0.05555555555555558\n",
      "Error Gain: 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def gini_index(groups, classes):\n",
    "    \"\"\"\n",
    "    Calculate the Gini index for a given dataset split.\n",
    "\n",
    "    Parameters:\n",
    "    - groups: A list of two groups of data after the split.\n",
    "    - classes: A list of unique classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - gini: The Gini index for the split.\n",
    "    \"\"\"\n",
    "    total_instances = sum(len(group) for group in groups)\n",
    "    gini = 0.0\n",
    "\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "\n",
    "        gini += (1.0 - score) * (size / total_instances)\n",
    "\n",
    "    return gini\n",
    "\n",
    "def misclassification_error(groups, classes):\n",
    "    \"\"\"\n",
    "    Calculate the Misclassification Error Rate for a given dataset split.\n",
    "\n",
    "    Parameters:\n",
    "    - groups: A list of two groups of data after the split.\n",
    "    - classes: A list of unique classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - error_rate: The Misclassification Error Rate for the split.\n",
    "    \"\"\"\n",
    "    total_instances = sum(len(group) for group in groups)\n",
    "    error_rate = 0.0\n",
    "\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        class_counts = Counter([row[-1] for row in group])\n",
    "        majority_class = max(class_counts, key=class_counts.get)\n",
    "        error_rate += (size - class_counts[majority_class])  # Corrected this line\n",
    "\n",
    "    return error_rate / total_instances\n",
    "\n",
    "def calculate_gain(current_gini, groups, classes, impurity_function):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a dataset split.\n",
    "\n",
    "    Parameters:\n",
    "    - current_gini: The Gini index (or Misclassification Error Rate) before the split.\n",
    "    - groups: A list of two groups of data after the split.\n",
    "    - classes: A list of unique classes in the dataset.\n",
    "    - impurity_function: Function to calculate impurity (either gini_index or misclassification_error).\n",
    "\n",
    "    Returns:\n",
    "    - gain: The information gain for the split.\n",
    "    \"\"\"\n",
    "    total_instances = sum(len(group) for group in groups)\n",
    "    weighted_impurity = sum(impurity_function([group], classes) * len(group) / total_instances for group in groups)\n",
    "    gain = current_gini - weighted_impurity\n",
    "    return gain\n",
    "\n",
    "# Test the implementations with a simple dataset\n",
    "# Example dataset: [(2, 0), (3, 0), (5, 1), (7, 1), (8, 1), (10, 0)]\n",
    "# Features: [2, 3, 5, 7, 8, 10]\n",
    "# Classes: [0, 1]\n",
    "dataset = [[2, 0], [3, 0], [5, 1], [7, 1], [8, 1], [10, 0]]\n",
    "classes = list(set(row[-1] for row in dataset))\n",
    "current_gini = gini_index([dataset], classes)\n",
    "current_error = misclassification_error([dataset], classes)\n",
    "\n",
    "# Split the dataset into two groups based on a feature\n",
    "split_feature_index = 0\n",
    "split_value = 6\n",
    "left_group = [row for row in dataset if row[split_feature_index] < split_value]\n",
    "right_group = [row for row in dataset if row[split_feature_index] >= split_value]\n",
    "\n",
    "# Calculate gains\n",
    "gini_gain = calculate_gain(current_gini, [left_group, right_group], classes, gini_index)\n",
    "error_gain = calculate_gain(current_error, [left_group, right_group], classes, misclassification_error)\n",
    "\n",
    "print(f\"Gini Gain: {gini_gain}\")\n",
    "print(f\"Error Gain: {error_gain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82945e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
